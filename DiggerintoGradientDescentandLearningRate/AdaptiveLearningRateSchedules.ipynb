{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Learning Rate Schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider two types of adaptive learning rate AdaGrad and RMSProp, they build on each other, so it won't be too much to take in.\n",
    "___\n",
    "\n",
    "#### AdaGrad\n",
    "\n",
    "AdaGrad is short for adaptive gradient algorithm, it dynamically varies the learning rate at each update and for each weight individually.\n",
    "\n",
    "The original rule was <code>w(t + 1) = w(t) - η * ∂l / ∂w * (t)</code>, the <code>w(t + 1)</code> means the next weight, the <code>w(t)</code> means the previous weight, the <code>η * ∂l / ∂w * (t)</code> means follow the updates rule.\n",
    "\n",
    "The change in <code>w(t + 1) - w(t) = - η * ∂l / ∂w * (t)</code>, so it will become <code>Δw = - η * ∂l / ∂w * (t)</code>, when we consider the adaptive gradient algorithm the change in the weight will be given by the same expression, but in addition it will be divided by the square root of G at t plus Epsilon.\n",
    "\n",
    "We begin at G not equal to 0 at each step G increases because we are adding to non-negative numbers, thus G is a monotonously increasing function, since we are dividing the learning rate <code>η</code> by a monotonously increasing function, <code>η</code> divided by that is obviously monotonously decreasing.\n",
    "\n",
    "The Epsilon is some small number we need to put there because if G is 0, we won't be albe to perform the division.\n",
    "\n",
    "It is basically a smart adaptive learning rate scheduler, adaptive stands for the fact that the effective learning rate is based on the training itself, it is not a pre-set learning schedule like the exponential one where all the learning rate values are calculated regardless of the training process, another very important point is that the adaptation is per weight, this means every individual weight in the whole network keeps track of its own G function to normalize its own steps, it's an important observation as different wieghts don't reach their optimal values simultaneously\n",
    "___\n",
    "\n",
    "#### RMSProp\n",
    "\n",
    "RMSProp is short for root mean square propagation, it is very similar to AdaGrad, the update rule is defined in the same way, but the G function is a bit different, the two terms are assigned weights <code>β</code> and <code>1 - β</code>, effective keeping track of a moving average of the G values, this new hyperparameter <code>β</code> is a number between 0 and 1, the value of 0.9 is very typical.\n",
    "\n",
    "The implication here is that the function is no longer monotonously increasing, hence <code>η</code> divided by the square root of G is not monotonously decreasing, empirical evidence shows that in this way the learning rate adapts much more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem with Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real life though loss function are not so regular, it was just one of its minimum, a local impostor rather than the sore extremum, so each local minimum is a suboptional solution to the machine learning optimization.\n",
    "\n",
    "Gradient descent is pron to this issue, often it falls into the close minimum to the starting point rather than the global minimun, of course it depends on the learning rate as well, a higher learning rate, may miss the first local minimum and fall directly into the global valley, however it is likely to oscillate and never reach it.\n",
    "\n",
    "So does this means the gradient descent optimization method is not almighty, not necessarily, remedies can be applied to reach the desired result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

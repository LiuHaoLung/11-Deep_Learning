{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate Schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters              |Parameters\n",
    "---                          |---\n",
    "Pre-set by us                |Found by optimizing\n",
    "Width                        |Weights(w)\n",
    "Depth                        |Biases(b)\n",
    "Learning rate                |-\n",
    "___\n",
    "\n",
    "The learning rate must be small enough, so we gently descend through the loss function instead of oscillating wildly around the minimum and never reaching it or diverging to infinity, it also had to be big enough so the optimization takes place in a reasonable amount of time.\n",
    "\n",
    "The small enough and big enough are too vague, a smart way to deal with choosing the proper learning rate is adopting a so-called learning rate schedule, learning rate schedules get the best of words, the small enough and big enough.\n",
    "\n",
    "The rationale is the following, we start from a high initial learning rate, this leads to faster training, in this way we approach the minimum faster, than we want to lower the rate gradually as training goes, around the end of the training, we want a very small learning rate so we get an accurate solution.\n",
    "___\n",
    "\n",
    "How are learning schedules implemented in practice:\n",
    "\n",
    "Step            |Piecewise constant learning rate\n",
    "---             |---\n",
    "We start from a high initial learning rate|First 5 epochs, learning rate is 0.1\n",
    "At some point we lower the rate to avoid oscillation|Next 5 epochs, learning rate is 0.01\n",
    "Around the end we pick a very small rate to get precise answer|Until the end, learning rate is 0.001\n",
    "\n",
    "There are two basic ways to do that, the simplest one is setting a pre-determined piecewise constant learning rate, this causes the loss function to converge much faster to the minimum and will give us an accurate result.\n",
    "\n",
    "Indeed it is crude as it requires us to know approximately how many epochs it will take the loss to converge, still beginners may want to use it as it makes a great difference compared to the constant learning rate.\n",
    "___\n",
    "\n",
    "A second much smarter approach is the exponential schedule, the exponential schedule is a much better alternative as it smoothly reduces or decays the learning rate, we usually start from a high value such as learning rate is 0.1, then we update the learning rate at each epoch using the rule like <code>η = η0 * e ^ (-n / c)</code>, n is the current epoch while c is a constant.\n",
    "\n",
    "So if the c is 20, the learning rate will be:\n",
    "\n",
    "Epoch             |Learning rate\n",
    "---               |---\n",
    "0                 |0.1\n",
    "1                 |0.0967\n",
    "2                 |0.0905\n",
    "3                 |0.0819\n",
    "4                 |0.0717\n",
    "5                 |0.0607\n",
    "6                 |0.0497\n",
    "7                 |0.0393\n",
    "8                 |0.0301\n",
    "\n",
    "There is no rule for the constant c but usually it shoulde be the same order of magnitude as the number of epochs needed to minimize the loss.\n",
    "\n",
    "For example if we need 100 epochs, values of c from 50 to 500 are all fine, if we need 1000 epochs, values of c from 500 to 5000 is fine, usually we'll need much less, so a value of c around 20 or 30 works well, c is yet another hyperparameter, as with all hyperparemeter is may make a differentce for you particular problem.\n",
    "\n",
    "The exact value of c doesn't matter as much, what makes a different is the presence of the lea rning schedule itself.\n",
    "___\n",
    "\n",
    "Hyperparameters              |Parameters\n",
    "---                          |---\n",
    "Width                        |Weights(w)\n",
    "Depth                        |Biases(b)\n",
    "Learning rate                |-\n",
    "Batch size                   |-\n",
    "Momentum coefficent          |-\n",
    "Decay coefficient            |-\n",
    "\n",
    "All those cool new improvements such as learning rate schedules and momentum come at a price, we pay the price of increasing the number of hyperparameters for which we must pick values.\n",
    "\n",
    "Generally the rule of thumb values work well but bear in mind that for some specific problem of yours they may not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two different optimizers, learning schedules and momentum, it would be even better if we can combine these concept and obtain even better results.\n",
    "\n",
    "Adam is short for adaptive moment estimation, it is the most advanced optimizer applied in practice.\n",
    "\n",
    "The AdaGrad and RMSprop did not include momemtum, Adam steps on RMSprop and introduces momemtum into the equation.\n",
    "\n",
    "So the update rule derived from RMSprop changes from <code>Δwi(t) = - η / ((Gi(t) ^ 2) + ε) * Mi(t)</code>, M is the momemtum but it is a bit transformed, <code>Mi(t) = αMi(t - 1) + (1 - α) * ∂l / ∂wi * (t)</code>, naturally Mi(0) = 0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The before example, that's a type of nerual network, but it has no depth, it is use the linear model to learn the function, this function was the best fit of the data according to the L2-norm loss with a couple of hundred iterations.\n",
    "\n",
    "But the most real life dependencies can't be modeled with a simple linear combination, because we want to be better forecasts so it will need better models, most of the time, this means working with a model that is more sophisticated than a linear model such complexity is usually achieved by using both linear and non-linear operations, mixing linear combinations and non-linearities allow us to model arbitrary functions or in other words functions with strange unconventional shapes.\n",
    "\n",
    "So basically the model changes from inputs that are linearly combined resulting in outputs to inputs that are linearly combined and then go through some nonlinear transformation resulting in outputs.\n",
    "\n",
    "The one a commonly uesd nonlinearity is the sigmoid function, it is defined as <code>Ïƒ(x) = 1 / (1 + e ^ -x)</code>,  the initial linear combination and the added nonlinearity form a layer, the layer is the building block of neural networks when it have more than one layer, it is taking about a deep neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

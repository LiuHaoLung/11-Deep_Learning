{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer is input layer, this is basically the data we have, we take the inputs and get outputs as we did before, the main rationale behind neural networks however is that we can now use these outputs as inputs for another layer and then another one and another until we decide to stop, the last layer we build is the output layer, that's basically what we compare the targets to.\n",
    "\n",
    "The first layer is the input layer and the last layer is the output layer, all the layer between are called hidden layers, we call them hidden, as we know the inputs and we get the outputs but we don't know what happens between as these operations are hidden, stacking layers one after the other produces a deep network or as we will call it a deep net.\n",
    "\n",
    "The building blocks of the hidden layer are called hidden units or nodes, so if the one is called hidden unit, in mathematical terms if h is a tensor related to the hidden layer, each hidden unit is an element of that tensor.\n",
    "\n",
    "The number of units(nodes) in layer is often reffered to as the width of the layer, usually but not always we stack with the same width so that layer width is equal to the width of the entire network.\n",
    "\n",
    "Depth is an important ingredient as it refers to the number of hidden layers in a network, when we create a machine learning algorithm, we choose it's width and depth, we refer to these values as hyper parameters, hyperparameters should not be mistaken with parameters.\n",
    "\n",
    "Hyperparameters              |Parameters\n",
    "---                          |---\n",
    "Width                        |Weights(w)\n",
    "Depth                        |Biases(b)\n",
    "Learning rate                |-\n",
    "\n",
    "The main difference between the two is that the value of the parameters will be derived through optimization while the hyperparameters are set by us before we start optimizing, so the hyperparameters is pre-set by us, the parameters is found by optimizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple example, the inputs are x1 and x2, the hidden layer is h1, h2 and h2, the outpus are y1 and y2, the targets are t1 and t2, the weights are w11, w12, w13, w21, w22 and w23 for the first part of the net, the second part it named u11, u12, u13, u21, u22 and u23, so it can differentiate between the two types of weights and it is important to understand that the weights for each layer a different.\n",
    "\n",
    "We know the error associated with y1 and y2 as it depends on known targets t1 and t2, so the errors are e1 and e2.\n",
    "\n",
    "Based on them, we can adjust the weights labeled with u, each u contributes to a single error, for example, u11 contributes to a single error e1, then we find its derivative(gradient) and update the coefficient.\n",
    "\n",
    "w11 help predict the h1 but then we needed h1 to calculate y1 and y2, thus it play a role in determining both errors e1 and e2.\n",
    "\n",
    "So while u11 contributes to a single error e1, w11 contributes to both errors, therefore it's adjustment rule must be different.\n",
    "\n",
    "The solution to this problem is to take the errors and backpropagation them throuhg the net using the weights(u), knowing the u weights, we can measure the contributio of each hidden unit to respective errors, then once we found out the contribution of each hidden unit to the respective errors, we can update the w weights.\n",
    "\n",
    "So essentially through backpropagation the algorithm identifies which weights lead to which errors, then it adjusts the weights that have a bigger contribution to the errors by more than the weights with smaller contribution to the errors by less.\n",
    "\n",
    "A big problem arises when we must also consider the activation functions, they introduce additional complexity to this process, linear contribution are easy, but non-linear ones are tougher, the backpropagation is one of the biggest challenges for the speed of an algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

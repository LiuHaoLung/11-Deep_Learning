{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning context, non-linearities are called activation functions, activation functions transform inputs into outputs of a different kind.\n",
    "\n",
    "Example, the input you got was the change in the temperature, the activation functions transformed this input into an action, put on the jactet or continue carrying it, this is also the output after the transformation, it is a binary variable jacket or no jacket.\n",
    "\n",
    "So this example is the basic logic behind non-linearities, the change in the temperature was following a linear model as it was steadily decreasing, the activation functions transformed this relationship into an output linked to the temperature but was of a different kink.\n",
    "\n",
    "In machine learning we have different activation functions but there are a few use much more frequently than others.\n",
    "\n",
    "In this table, the derivative is an essential part of the gradient descent, when we work with tensorflow, it's calculated automatically.\n",
    "\n",
    "\n",
    "Name                             |formula     |Derivative           |Range\n",
    "---                              |---         |---                  |---\n",
    "Sigmoid(Logistic function)       |<code>σ(a) = 1 / (1 + e ^ -a)</code>|<code>∂σ(a) / ∂a = σ(a)(1 - σ(a))</code>|(0,1)\n",
    "TanH(Hyberbolic tangent)         |<code>tanh(a) = (e ^ a - e ^ -a) / (e ^ a + e ^ -a)</code>|<code>∂tanh(a) / ∂a = 4 / (e ^ a + e ^ -a) ^ 2</code>|(-1,1)\n",
    "ReLu(rectified linear unit)      |<code>relu(a) = max(0,a)</code>|<code>∂relu(a) / ∂a = 0, if a<=0, 1 if a > 0</code>|<code>(0,∞)</code>\n",
    "    softmax                          |<code>σi(a) = e ^ ai / Σi * (e ^ aj)</code>|<code>∂σi(a) / ∂aj = σi(a)(δij - σ(a)), when δij is 1 if i = j, 0 otherwise</code>|(0,1)\n",
    "    \n",
    "The all activation functions are all monotonic, continuous and differentiable, this are important properties needed for the optimization process.\n",
    "\n",
    "Activation functions are also called transfer functions, because of the transformation properties, the two terms are used interchangeably in machine learning context, but different in other fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmac activaiton functions has no define graph, while this function is different if we take a careful look at its formula <code>σi(a) = e ^ ai / Σi * (e ^ aj)</code>, the key difference between this function and the other is it takes an argument the whole vector a instead of individual elements, so the softmax function is equal to the exponential of the element at position i divided by the sum of the exponential of all elements of the vector, the softmax considers the information about the whole set of numbers we have.\n",
    "\n",
    "The softmax is different, each element in the output depend on the entire set of elements of the input.\n",
    "\n",
    "A key aspect of the softmax transformation is that the values it outputs are in the range from 0 to 1, the sum of outputs are exactly 1 and probabilities.\n",
    "\n",
    "The point of the softmax transformation is to transform a bunch of arbitrarily large or small numbers into a valid probability distribution.\n",
    "\n",
    "Softmax is often used as the activation of the output layer in classification problems, so no matter what happens before the final output of the algorithm is a probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Algorithm n- Parameter Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the update rule:\n",
    "\n",
    "\n",
    "$$x_{i+1} = x_i - eta * f'(x_i)$$\n",
    "\n",
    "\n",
    "With the multidimensional space, the update rule become:\n",
    "\n",
    "\n",
    "$$w_{i+1} = w_i - eta * ∇w * L(y,t)$$\n",
    "$$b_{i+1} = b_i - eta * ∇b * L(y,t)$$\n",
    "\n",
    "\n",
    "The <code>∇</code> means the gradient of the loss function, the w is for the weight, the b is for the bias.\n",
    "\n",
    "We want to minimize the loss function by varying the weights and the biases, this means we are trying to optimize the loss function regarding w and b.\n",
    "\n",
    "\n",
    "Mathematically it looks like this:\n",
    "\n",
    "\n",
    "$$∇w * L(y,t) = Σ_i * ∇w *\\frac{1}{2} * (y_i - t_i) ^ 2$$\n",
    "\n",
    "\n",
    "From the linear model:\n",
    "\n",
    "\n",
    "$$y_i = wx + b$$\n",
    "\n",
    "\n",
    "So it becomes:\n",
    "\n",
    "\n",
    "$$∇w * L(y,t) = Σ_i * ∇w * δ_i$$\n",
    "\n",
    "\n",
    "The <code>δ</code> is often used to measure differences, so we calculate that expression for each observation and then sum them all.\n",
    "\n",
    "Logically the gradient of the loss function with respect to the bias:\n",
    "\n",
    "\n",
    "$$∇b * L(y,t) = Σ_i * δ_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Algorithm 1- Parameter Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest and the most fundamental optimization algorithm is the gradient descent, the gradient is the multivariate generalization of the derivatibe concept.\n",
    "\n",
    "Consider a non machine learning example to know the logic behind the gradient descent.\n",
    "\n",
    "There is a equation: $$f(x) = 5x^2 + 3x - 4$$\n",
    "\n",
    "\n",
    "Our goal is to find the mininum of this function using the gradient descent methodology, the first step is to find the first dervative of the function:\n",
    "\n",
    "\n",
    "$$f'(x) = 10x + 3$$\n",
    "\n",
    "\n",
    "The second step would be choose any arbitrary number, for example:\n",
    "\n",
    "\n",
    "$$x_0 = 4$$\n",
    "\n",
    "\n",
    "Then we calculate a different number:\n",
    "\n",
    "\n",
    "$$x_1$$ \n",
    "\n",
    "\n",
    "Following the update rule:\n",
    "\n",
    "\n",
    "$$x_{i+1} = x_i - ηf'(x_i)$$\n",
    "\n",
    "\n",
    "So the $$x_1 = 4 - η[10*4 + 3] = 4 - η43$$\n",
    "\n",
    "\n",
    "<code>η(eta)</code> is learning rate, it is the rate at which the machine learning algorithm forgets old beliefs for new ones.\n",
    "\n",
    "Using the update rule, it can find:\n",
    "\n",
    "$$x_2 = x_1 - η f'(x_1)$$\n",
    "\n",
    "\n",
    "$$x_3 = x_2 - η f'(x_2)$$\n",
    "\n",
    "\n",
    "After conducting the update operation long enough, the values will eventually stop updating, when is stop updating, that is the point which we know we have reached the mininim of the function, this is because the first derivative of the function is 0 when we have reached the minimum.\n",
    "\n",
    "\n",
    "So the function:\n",
    "\n",
    "\n",
    "$$x_{i+1} = x_i - ηf'(x_i)$$\n",
    "\n",
    "\n",
    "It will become the:\n",
    "\n",
    "\n",
    "$$x_{i+1} = x_i$$\n",
    "\n",
    "\n",
    "Therefore the update rule will no longer update, once the minimun is reached, all subsequent values are equal to it and the speed of minimization depends on the <code>η.</code>\n",
    "\n",
    "Generally we want the learning rate to be high enough, so we can reach the closet minimum in a rational amount of time, in this function, the <code>η = 0.001</code> was too small, at the same time, we want <code>η</code> to be lower enough, so we don't oscillate around the minimum.\n",
    "___\n",
    "\n",
    "Using the gradient descent, we can find the minimum value of a function through a trial and error method; second there is an update rule that allows us to cherry pick the trials so we can reach the minimum, that means each trial is better than the previous one; third we must think about the learning range which has to be high enough so we don't iterate forever, but low enough so we don't oscillate forever; finally once we have converged we should stop updating, one way to know we have converged is when the difference between the term at place:\n",
    "\n",
    "\n",
    "$$x_{i+1} - x_i = 0.001$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

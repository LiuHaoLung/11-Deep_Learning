{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early Stopping & When to Stop Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early stopping\n",
    "\n",
    "Early stopping is a technique to prevent overfitting, it is called early stopping as we want to stop early before we overfit.\n",
    "___\n",
    "\n",
    "#### First strategy - preset number of epoch\n",
    "\n",
    "The simplest one is train for the pre-set number of epochs, in the minimal example, we train for 100 epochs, the pros is eventually it can solves the problem, but the cons it no guarantee that the minimum has been reached or passed, a high enough learning rate would even cause the loss to diversed to infinity.\n",
    "___\n",
    "\n",
    "#### Second strategy - updates to small\n",
    "\n",
    "A bit more sophisticated technique is to stop when the loss function updates becomes sufficiently small, a common rule of thumb is to stop when the relative decrease in the loss function becomes less than 0.001 or 0.1, this simple rule has two underlying ideas, the pros is we are sure we won't stop before we have reached a minumum, that's because of the way gradient descent works, it will descend until a minimum is reached, so the loss function stops changing, making the update rule yield in the same weights, the second is that we want to save computing power by using as few iterations as possible, as it said once we have reached the minimum or diverged to infinity, we will be stuck there, knowing that a gazillion more epochs won't change a thing, we can just stop there, this saves us the trouble of iterating uselessly without updating anything.\n",
    "\n",
    "\n",
    "Mean                            |Perset epochs      |Updates too small\n",
    "---                             |---                |---\n",
    "Solves the problem              |Yes                |Yes\n",
    "Certain that loss is minimized  |No                 |Yes\n",
    "Doesn't iterate uselessly       |No                 |Yes\n",
    "Prevents overfitting            |No                 |No\n",
    "\n",
    "In the pre-set epochs, may ultimately minimize the loss, chances are we can't guess the number of required epochs, probably the algorithms would have performed thousands of iterations that did not update the weights, obviously each epoch that changes nothing is uesless and should be dropped. \n",
    "___\n",
    "\n",
    "#### Third strategy - validation\n",
    "\n",
    "As time goes by the error becomes smaller, the distribution is exponential, as initially we are finding better weights quickly, the more we train the model, the harder it gets to achieve an improvement, at some point it becomes almost flat, now if we put the validation curve on the same graph, it would start with the training cost, at the point when we start overfitting, the validation cost will start increasing, here's the point I wish the two function begin diverging, so in this situation, we should stop the algorithm before we do more damage to the model.\n",
    "\n",
    "The validation strategy pros is we are sure the validation loss is minimized, save the computing power and prevents overfitting.\n",
    "\n",
    "Mean                            |Perset epochs      |Updates too small    |Validation set\n",
    "---                             |---                |---                  |---\n",
    "Solves the problem              |Yes                |Yes                  |Yes\n",
    "Certain that loss is minimized  |No                 |Yes                  |Yes\n",
    "Doesn't iterate uselessly       |No                 |Yes                  |No\n",
    "Prevents overfitting            |No                 |No                   |Yes\n",
    "\n",
    "So stop when the validation loss starts increasing or when the training loss becomes very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

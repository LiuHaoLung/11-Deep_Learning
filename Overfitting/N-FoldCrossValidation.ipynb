{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be split the dato into three parts, training, validation and test, this is a standard mechanism and usually when machine learning is appropriate, we have enough data to apply it.\n",
    "\n",
    "What if we have a small data set? we can't afford to split it into three datasets as we will lose some of the underlying relationships or worse, we can have so little data left for training that the algorithm can't learn anything, there is another answer to this issue and it's called n-fold cross validation, sometimes called k-fold cross validation.\n",
    "\n",
    "This is a strategy that resembles the general one but combines the train and validation data sets in a clever way, however it still requires a test subset, we're combining the training and validation steps but we can't avoid the test stage.\n",
    "\n",
    "Example there is a dataset containing 11000 observations, we'll save 1000 observations for the test, what we are left with are 10000 samples, this example is not very big.\n",
    "\n",
    "In this case, we want to train on 9000 data points and validate on 1000, we'll split the remaining data into 10 subsets containing 1000 observations each, we fold it 10 times, so this is a 10-fold cross-validation, 10 is also a commonly used value and that's why we picked it for this illustration.\n",
    "\n",
    "We treat 1 subset(1000 observations) as a validation set while the ohter 9 combinded as a training set, during the first epoch, the first chunk of data serves as validation, then in the second epoch, the second chunk of data serves as validaion and so on, so in this way for each epoch, we don't overlap training and validation as it should be.\n",
    "\n",
    "Moreover we managed to use the whole dataset except for the test part, as with all good things, this comes at a price we have still trained on the validation set which was not a good idea, it is less likely that the overfitting flag is raised and it is possible that we all were fitted a bit.\n",
    "\n",
    "The tradeoff is between not having a model or having a model that's a bit overfitted and fold cross-validation solves the scarce data issue but should by no means be used as the norm.\n",
    "\n",
    "Whenever you can, divid the data into three part, training, validation and test, only if it doesn't manage to learn much because of data scarce it you should try the n-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

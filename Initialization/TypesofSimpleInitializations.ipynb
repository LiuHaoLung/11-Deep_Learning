{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Simple Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First approach\n",
    "\n",
    "A simple approach would be to initialize weights randomly within a small range, jusk like in the example, use the np method random.uniform and the range was between -0.1 and 0.1, this approach chooses the values randomly, but in a uniform manner, each one has the exact same probability of being chosen, equal probability of being selected sounds intuitive but it is important to stress it.\n",
    "___\n",
    "\n",
    "#### Second approach\n",
    "\n",
    "We could choose a normal initialiser, the idea is basically the same, this time though we pick the number from 0 mean normal distribution, the chosen variance is arbitrary but should be small, since follows the normal distribution values close to 0 are much more likely to been chosen than other values.\n",
    "\n",
    "An example of such initialization is to draw from a normal distribution with a mean 0 and the standard deviation 0.1, both methods are somewhat problematic although they were the norm until 2010, it was just recently that academics came up with a solution.\n",
    "\n",
    "Weights are used in linear combinations then the linear combinations are activated, once more we will use the sigmoid avtivator, the sigmoid as other commonly uesd non-linearities is peculiar around its mean and its extreme's, activation functions take as inputs linear combination of the units from the previous layer, well if the weights are too small this will cause valuers that fall around this range, in this range unfortunately the sigmoid is almost linear, if all our inputs are in this range which will happen if we use small weights, the sigmoid would not apply non-linearity, but a linearity to the linear combination, as we discussed non-linearities are essential for deep net.\n",
    "\n",
    "Conversely if the values are too large or too small, the sigmoid is almost flat which causes the output of the sigmoid to be only 1 or only 0 respectively, a static output of the activations minimises the gradient, well the algorithm is not really trained, so what we want is a wide range of inputs for the sigmoid.\n",
    "\n",
    "These inputs depend on the weights so the weights will hsve to be initialized in a reasonable range, so we have a nice variance along the linear combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
